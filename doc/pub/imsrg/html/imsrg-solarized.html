<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Deep Learning and convergence of many-body methods">

<title>Deep Learning and convergence of many-body methods</title>


<link href="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<link href="https://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
code { padding: 0px; background-color: inherit; }
pre {
  border: 0pt solid #93a1a1;
  box-shadow: none;
}

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Recurrent neural networks: Overarching view',
               2,
               None,
               '___sec0'),
              ('Set up of an RNN', 2, None, '___sec1'),
              ('A layer of recurrent neurons (left), unrolled through time '
               '(right)',
               2,
               None,
               '___sec2'),
              ('Performing IMSRG calculations with RNNs', 2, None, '___sec3'),
              ('Defining the inputs', 2, None, '___sec4'),
              ('Using the upper left diagonal matrix element to predict',
               2,
               None,
               '___sec5'),
              ('Using a $tanh$ activation function', 2, None, '___sec6'),
              ('Using a RELU activation function', 2, None, '___sec7'),
              ('Coupled Cluster theory', 2, None, '___sec8')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->



<center><h1>Deep Learning and convergence of many-body methods</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University, USA</b></center>
<center>[2] <b>Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</b></center>
<br>
<p>
<center><h4>Oct 1, 2019</h4></center> <!-- date -->
<br>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec0">Recurrent neural networks: Overarching view </h2>

<p>
There is much focus on feedforward neural networks, ncluding
convolutional neural networks as well.  The output or the activations
flow only in one direction, from the input layer to the output layer.

<p>
A recurrent neural network (RNN) looks very much like a feedforward
neural network, except that it also has connections pointing
backward.

<p>
RNNs are used to analyze time series data such as stock prices, and
tell you when to buy or sell. In autonomous driving systems, they can
anticipate car trajectories and help avoid accidents. More generally,
they can work on sequences of arbitrary lengths, rather than on
fixed-sized inputs like all the nets we have discussed so far. For
example, they can take sentences, documents, or audio samples as
input, making them extremely useful for natural language processing
systems such as automatic translation and speech-to-text.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec1">Set up of an RNN </h2>

<p>
The figure here displays a simple example of an RNN, with inputs \( x_t \)
at a given time \( t \) and outputs \( y_t \).  Introducing time as a variable
offers an intutitive way of understanding these networks. In addition
to the inputs \( x_t \), the layer at a time \( t \) receives also as input
the output from the previous layer \( t-1 \), that is \( y_{t1} \).

<p>
This means also that we need to have weights that link both the inputs \( x_t \) to the outputs \( y_t \) as well as weights that link
the output from the previous time \( y_{t-1} \) and \( y_t \).   The figure here shows an example of a simple RNN.

<p>
<br /><br /><center><p><img src="figures/rnn.png" align="bottom" width=700></p></center><br /><br />

<p>
<!-- !split  -->

<h2 id="___sec2">A layer of recurrent neurons (left), unrolled through time (right) </h2>

<p>
You can easily create a layer of recurrent neurons. At each time step t, every neuron receives both the
input vector \( x(t) \) and the output vector from the previous time step \( y(t&#8211;1) \), as shown in figure here.

<p>
<br /><br /><center><p><img src="figures/rnnlayer.png" align="bottom" width=700></p></center><br /><br />

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec3">Performing IMSRG calculations with RNNs </h2>

<p>
Calculational details:

<ol>
<li> We used a simple pairing model with four particles and four doubly degenerate levels, see LNP 936, chapter 8, 10 and 11.</li>
<li> We run an IMSRG calculation (Magnus expansion with \( s_{\mathrm{max}}=10 \), step size $ds=0.001$$, White generator</li>
<li> The training was done for \( s\in [0,0.5] \) and testing for \( s \in [0.5,10] \). Training data evenly spaced.</li>
<li> Activation functions are <b>tanh</b> and <b>RELU</b> and we used RNN and LSTM (Long short-term memory networks). They both had the same number of units in each layer, so the networks were structured like this:</li>

<ul>
   <li> input --> recurrent hidden layer (1000 units) --> recurrent hidden layer (100 units) --> dense hidden layer (10 units) --> output (1 unit)</li>
</ul>

<li> We used MSE as the loss function and Adam as the optimizer method. Finally, the networks both trained for 5000 epochs with a batch size of 28.</li> 
</ol>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec4">Defining the inputs </h2>

<p>
In the IMSRG we are transforming a matrix via repeated applications of unitary transformation to diagonal form, that is we have 
at \( s=0 \) a dense matrix

$$
\mathbf{A}=    \begin{bmatrix} a_{11}(s=0) & a_{12}(s=0) & a_{13}(s=0)   & \dots    & \dots  &\dots     & a_{1n}(s=0) \\
                                a_{21}(s=0) & a_{22}(s=0)& a_{23}(s=0) & \dots    & \dots  &\dots     &\dots \\
                                a_{31}(s=0)   & a_{32}(s=0) & a_{33}(s=0) & a_{34}(s=0)  &\dots       &\dots & \dots \\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                a_{(n-1)1}(s=0)   & \dots & \dots & \dots  &\dots a_{(n-1)(n-2)}(s=0)      &a_{(n-1)(n-1)}(s=0) & a_{(n-1)n}(s=0)\\
                                a_{n1}(s=0)   & \dots & \dots & \dots  &\dots       &a_{n(n-1)1}(s=0) & a_{nn}(s=0)\end{bmatrix},
$$

<p>
and for the next \( s \) value we may have

$$
\mathbf{\tilde{A}}=    \begin{bmatrix} a_{11}(s=1) & a_{12}(s=1) & 0   & 0    & \dots  &0     & 0 \\
                                a_{21}(s=1) & a_{22}(s=1)& a_{23}(s=1) & 0    & \dots  &0     &0 \\
                                0   & a_{32}(s=1) & a_{33}(s=1) & a_{34}(s=1)  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots a_{(n-1)(n-2)}(s=1)      &a_{(n-1)(n-1)}(s=1) & a_{(n-1)n}(s=1)\\
                                0   & \dots & \dots & \dots  &\dots       &a_{n(n-1)1}(s=1) & a_{nn}(s=1)\end{bmatrix},
$$

<p>
and finally we have

$$
\mathbf{D}=    \begin{bmatrix} a_{11}(s=\infty) & 0 & 0   & 0    & \dots  &0     & 0 \\
                                0 & a_{22}(s=\infty)& 0 & 0    & \dots  &0     &0 \\
                                0   & 0 & a_{33}(s=\infty) & 0  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &a_{(n-1)(n-1)}(s=\infty) & \\
                                0   & \dots & \dots & \dots  &\dots       &0 & a_{nn}(s=\infty)\end{bmatrix}.
$$

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec5">Using the upper left diagonal matrix element to predict </h2>

<p>
We then use the upper left diagonal matrix element as our input to the training, that is we run training and testing as

<ol>
<li> \( a_{11}(s=s_0=0) \), \( a_{11}(s_1) \) and \( a_{11}(s_2) \) to predict \( a_{11}(s_3) \)</li>  
<li> Then we use \( a_{11}(s_1) \), \( a_{11}(s_2) \) and \( a_{11}(s_3) \) to predict \( a_{11}(s_4) \)</li>    
<li> continue till we reach \( s=10 \).</li> 
</ol>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec6">Using a \( tanh \) activation function  </h2>

<p>
<br /><br /><center><p><img src="figures/predictions_tanh.png" align="bottom" width=700></p></center><br /><br />

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec7">Using a RELU activation function  </h2>

<p>
<br /><br /><center><p><img src="figures/predictions_relu.png" align="bottom" width=700></p></center><br /><br />

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec8">Coupled Cluster theory </h2>

<p>
We have also performed Coupled Cluster and used the correlation energy calculated for the same pairing model with exact CCD calculations 
up to N=12$ particles. RNNs have been used in the same way to extrapolate to \( N=40 \), with excellent results.

<p>

<!-- ------------------- end of main content --------------- -->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2019, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

