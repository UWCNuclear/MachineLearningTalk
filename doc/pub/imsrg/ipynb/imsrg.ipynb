{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Deep Learning and convergence of many-body methods -->\n",
    "# Deep Learning and convergence of many-body methods\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen at Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University, USA & Department of Physics and Center for Computing in Science Education, University of Oslo, Norway -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen**, Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University, USA and Department of Physics and Center for Computing in Science Education, University of Oslo, Norway\n",
    "\n",
    "Date: **Oct 1, 2019**\n",
    "\n",
    "Copyright 1999-2019, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Recurrent neural networks: Overarching view\n",
    "\n",
    "There is much focus on feedforward neural networks, ncluding\n",
    "convolutional neural networks as well.  The output or the activations\n",
    "flow only in one direction, from the input layer to the output layer.\n",
    "\n",
    "A recurrent neural network (RNN) looks very much like a feedforward\n",
    "neural network, except that it also has connections pointing\n",
    "backward. \n",
    "\n",
    "RNNs are used to analyze time series data such as stock prices, and\n",
    "tell you when to buy or sell. In autonomous driving systems, they can\n",
    "anticipate car trajectories and help avoid accidents. More generally,\n",
    "they can work on sequences of arbitrary lengths, rather than on\n",
    "fixed-sized inputs like all the nets we have discussed so far. For\n",
    "example, they can take sentences, documents, or audio samples as\n",
    "input, making them extremely useful for natural language processing\n",
    "systems such as automatic translation and speech-to-text.\n",
    "\n",
    "\n",
    "## Set up of an RNN\n",
    "\n",
    "The figure here displays a simple example of an RNN, with inputs $x_t$\n",
    "at a given time $t$ and outputs $y_t$.  Introducing time as a variable\n",
    "offers an intutitive way of understanding these networks. In addition\n",
    "to the inputs $x_t$, the layer at a time $t$ receives also as input\n",
    "the output from the previous layer $t-1$, that is $y_{t1}$.\n",
    "\n",
    "This means also that we need to have weights that link both the inputs $x_t$ to the outputs $y_t$ as well as weights that link\n",
    "the output from the previous time $y_{t-1}$ and $y_t$.   The figure here shows an example of a simple RNN.\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [figures/rnn.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<p></p>\n",
    "<img src=\"figures/rnn.png\" width=700>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- !split  -->\n",
    "## A layer of recurrent neurons (left), unrolled through time (right)\n",
    "\n",
    "You can easily create a layer of recurrent neurons. At each time step t, every neuron receives both the\n",
    "input vector $x(t)$ and the output vector from the previous time step $y(tâ€“1)$, as shown in figure here. \n",
    "\n",
    "<!-- dom:FIGURE: [figures/rnnlayer.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<p></p>\n",
    "<img src=\"figures/rnnlayer.png\" width=700>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Performing IMSRG calculations with RNNs\n",
    "\n",
    "Calculational details:\n",
    "1. We used a simple pairing model with four particles and four doubly degenerate levels, see LNP 936, chapter 8, 10 and 11.\n",
    "\n",
    "2. We run an IMSRG calculation (Magnus expansion with $s_{\\mathrm{max}}=10$, step size $ds=0.001$$, White generator\n",
    "\n",
    "3. The training was done for $s\\in [0,0.5]$ and testing for $s \\in [0.5,10]$. Training data evenly spaced.\n",
    "\n",
    "4. Activation functions are **tanh** and **RELU** and we used RNN and LSTM (Long short-term memory networks). They both had the same number of units in each layer, so the networks were structured like this:\n",
    "\n",
    "   * input --> recurrent hidden layer (1000 units) --> recurrent hidden layer (100 units) --> dense hidden layer (10 units) --> output (1 unit)\n",
    "\n",
    "\n",
    "5. We used MSE as the loss function and Adam as the optimizer method. Finally, the networks both trained for 5000 epochs with a batch size of 28. \n",
    "\n",
    "## Defining the inputs\n",
    "\n",
    "In the IMSRG we are transforming a matrix via repeated applications of unitary transformation to diagonal form, that is we have \n",
    "at $s=0$ a dense matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}=    \\begin{bmatrix} a_{11}(s=0) & a_{12}(s=0) & a_{13}(s=0)   & \\dots    & \\dots  &\\dots     & a_{1n}(s=0) \\\\\n",
    "                                a_{21}(s=0) & a_{22}(s=0)& a_{23}(s=0) & \\dots    & \\dots  &\\dots     &\\dots \\\\\n",
    "                                a_{31}(s=0)   & a_{32}(s=0) & a_{33}(s=0) & a_{34}(s=0)  &\\dots       &\\dots & \\dots \\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                a_{(n-1)1}(s=0)   & \\dots & \\dots & \\dots  &\\dots a_{(n-1)(n-2)}(s=0)      &a_{(n-1)(n-1)}(s=0) & a_{(n-1)n}(s=0)\\\\\n",
    "                                a_{n1}(s=0)   & \\dots & \\dots & \\dots  &\\dots       &a_{n(n-1)1}(s=0) & a_{nn}(s=0)\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for the next $s$ value we may have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\tilde{A}}=    \\begin{bmatrix} a_{11}(s=1) & a_{12}(s=1) & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                a_{21}(s=1) & a_{22}(s=1)& a_{23}(s=1) & 0    & \\dots  &0     &0 \\\\\n",
    "                                0   & a_{32}(s=1) & a_{33}(s=1) & a_{34}(s=1)  &0       &\\dots & 0\\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots a_{(n-1)(n-2)}(s=1)      &a_{(n-1)(n-1)}(s=1) & a_{(n-1)n}(s=1)\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &a_{n(n-1)1}(s=1) & a_{nn}(s=1)\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{D}=    \\begin{bmatrix} a_{11}(s=\\infty) & 0 & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                0 & a_{22}(s=\\infty)& 0 & 0    & \\dots  &0     &0 \\\\\n",
    "                                0   & 0 & a_{33}(s=\\infty) & 0  &0       &\\dots & 0\\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &a_{(n-1)(n-1)}(s=\\infty) & \\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &0 & a_{nn}(s=\\infty)\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the upper left diagonal matrix element to predict\n",
    "\n",
    "We then use the upper left diagonal matrix element as our input to the training, that is we run training and testing as\n",
    "1. $a_{11}(s=s_0=0)$, $a_{11}(s_1)$ and $a_{11}(s_2)$ to predict $a_{11}(s_3)$  \n",
    "\n",
    "2. Then we use $a_{11}(s_1)$, $a_{11}(s_2)$ and $a_{11}(s_3)$ to predict $a_{11}(s_4)$    \n",
    "\n",
    "3. continue till we reach $s=10$. \n",
    "\n",
    "## Using a $tanh$ activation function\n",
    "\n",
    "<!-- dom:FIGURE: [figures/predictions_tanh.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<p></p>\n",
    "<img src=\"figures/predictions_tanh.png\" width=700>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "## Using a RELU activation function\n",
    "\n",
    "<!-- dom:FIGURE: [figures/predictions_relu.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<p></p>\n",
    "<img src=\"figures/predictions_relu.png\" width=700>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Coupled Cluster theory\n",
    "\n",
    "We have also performed Coupled Cluster and used the correlation energy calculated for the same pairing model with exact CCD calculations \n",
    "up to N=12$ particles. RNNs have been used in the same way to extrapolate to $N=40$, with excellent results."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
